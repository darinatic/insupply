{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b795ed-1e53-45c1-b06e-679f599e9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# Use tkinter to select the Excel file\n",
    "def select_file():\n",
    "    Tk().withdraw()\n",
    "    file_path = askopenfilename(\n",
    "        title=\"Select Training Excel File\",\n",
    "        filetypes=[(\"Excel files\", \"*.xlsx *.xls\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "# Prompt the user\n",
    "print(\"Please select the training data file:\")\n",
    "training_excel_file = select_file()\n",
    "\n",
    "if training_excel_file:\n",
    "    # Read Excel file\n",
    "    df_training = pd.read_excel(training_excel_file, sheet_name='Sheet1')\n",
    "    \n",
    "    print(\"Top 5 rows in the training DataFrame:\")\n",
    "    print(df_training.head())\n",
    "else:\n",
    "    print(\"No file selected. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b4a71-f5a1-4a3f-87ba-c1de0b503ebc",
   "metadata": {},
   "source": [
    "**Data cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147fd51-4c2d-4a25-9825-611d93623152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load materials.json\n",
    "with open('../data/processed/materials.json', 'r') as file:\n",
    "    materials_data = json.load(file)\n",
    "\n",
    "# Convert materials.json into a lookup dictionary for efficient matching\n",
    "materials_lookup = {item['material_number']: item['description'] for item in materials_data}\n",
    "\n",
    "# Ensure the necessary columns exist\n",
    "if 'Combined Description' not in df_training.columns or 'Commitment item' not in df_training.columns:\n",
    "    raise ValueError(\"The training Excel file must contain 'Combined Description' and 'Commitment item' columns.\")\n",
    "\n",
    "# Data cleansing: Remove null values and strip whitespace\n",
    "df_training = df_training.dropna(subset=['Combined Description', 'Commitment item'])\n",
    "df_training['Combined Description'] = df_training['Combined Description'].str.strip()\n",
    "\n",
    "# Extract material_number from 'Commitment item' and map to descriptions\n",
    "df_training['material_number'] = df_training['Commitment item'].str.split(' - ', expand=True)[0]\n",
    "df_training['description'] = df_training['material_number'].map(materials_lookup)\n",
    "\n",
    "# Drop the original 'Commitment item' column\n",
    "df_training = df_training.drop(columns=['Commitment item'])\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"\\nCleaned training DataFrame:\")\n",
    "print(df_training[['material_number', 'description', 'Combined Description']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fe733-be85-4538-a676-dad5fe9f061f",
   "metadata": {},
   "source": [
    "**Vector embedding of materials.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea8ae6-23ba-4aa8-a4f7-2fa808e727ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src import search\n",
    "json_file = '../data/processed/services.json'\n",
    "\n",
    "# Initialize the search engine with reference data\n",
    "search_engine = search.SentenceTransformerSearch(data_file=json_file)\n",
    "\n",
    "print(\"\\nVector embedding completed for reference data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7452d58",
   "metadata": {},
   "source": [
    "**Evaluate model, save to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.search import evaluate_model\n",
    "\n",
    "# Split the data \n",
    "train_df, test_df = train_test_split(df_training, test_size=0.5, random_state=42)\n",
    "\n",
    "# Evaluate the model    \n",
    "results = evaluate_model(search_engine, train_df, top_k=5)\n",
    "\n",
    "# Convert the results to a DataFrame \n",
    "df_default = pd.DataFrame(results)\n",
    "df = df_default\n",
    "print(\"\\nEvaluation completed. Results DataFrame:\")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989392c-98aa-450f-abf7-2a230e673313",
   "metadata": {},
   "source": [
    "## Analysis 2: which service numbers are performing well and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca8bfa-f375-4789-888e-98b00fcd0ee4",
   "metadata": {},
   "source": [
    "**for the top performers compute precision as well as the correct/incorrect predictions**<br>\n",
    "How metrics are previously calculated:<br>\n",
    "In the previous calculations, even if a record is classified as \"correct\" for one of the top 5 values matching the expected service number, any other service numbers present in the top 5 that are not the expected service number will contribute to their own incorrect count.<br>\n",
    "\n",
    "This may not make much sense as it might inflate the model's incorrectness <br>\n",
    "\n",
    "How they are calculated now:<br>\n",
    "If in one record, any of the top 5 matches the expected, then it would not add to the incorrect count for the other 4 service numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccd7f3-8fd5-4e77-b2da-1a458d3f66a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML  \n",
    "\n",
    "# Flatten the data into a DataFrame\n",
    "df_copy = pd.DataFrame([\n",
    "    {\n",
    "        \"query\": entry[\"query\"],\n",
    "        \"expected\": entry[\"expected\"],\n",
    "        \"expected_description\": entry[\"expected_description\"],  \n",
    "        \"retrieved_material_number\": retrieved[\"material_number\"],\n",
    "        \"retrieved_description\": retrieved[\"description\"],\n",
    "        \"score\": retrieved[\"score\"],\n",
    "        \"is_correct\": retrieved[\"material_number\"] == entry[\"expected\"]\n",
    "    }\n",
    "    for entry in df.to_dict(\"records\") \n",
    "    for retrieved in entry[\"retrieved_top_5\"]\n",
    "])\n",
    "\n",
    "'''\n",
    "Why True Negatives Don't Apply\n",
    "In this context:\n",
    "\n",
    "- The model is not tasked with determining what doesnâ€™t belong.\n",
    "- There is no explicit prediction or evaluation for items outside the top 5.\n",
    "- Therefore, true negatives are not meaningful.\n",
    "'''\n",
    "\n",
    "# Exclude matches if any in the top 5 is correct\n",
    "df_copy['query_correct'] = df_copy.groupby('query')['is_correct'].transform('max')\n",
    "\n",
    "# Calculate metrics for each service number, including hit rate\n",
    "service_metrics = []\n",
    "for material_number, group in df_copy.groupby(\"retrieved_material_number\"):\n",
    "    total_correct = group[\"is_correct\"].sum()\n",
    "    total_incorrect = group[(group['query_correct'] == 0)].shape[0]\n",
    "    false_negatives = len(df_copy[(df_copy[\"expected\"] == material_number) &\n",
    "                                  (~df_copy[\"retrieved_material_number\"].eq(material_number))])\n",
    "\n",
    "    precision = total_correct / (total_correct + total_incorrect) if (total_correct + total_incorrect) > 0 else 0\n",
    "    accuracy = total_correct / len(group)\n",
    "    #accuracy = total_correct / (total_correct + total_incorrect + false_negatives) if (total_correct + total_incorrect + false_negatives) > 0 else 0\n",
    "    hit_rate = len(group)\n",
    "\n",
    "    description = group[\"retrieved_description\"].iloc[0]\n",
    "\n",
    "    # Append metrics for this service number\n",
    "    service_metrics.append({\n",
    "        \"service_number\": material_number, \n",
    "        \"hit_rate\": hit_rate,\n",
    "        \"total_correct\": total_correct,\n",
    "        \"total_incorrect\": total_incorrect,\n",
    "        \"precision\": round(precision, 3),  \n",
    "        \"accuracy\": round(accuracy, 3),   \n",
    "        \"description\": description\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(service_metrics)\n",
    "metrics_df = metrics_df.sort_values(by=\"precision\", ascending=False)\n",
    "\n",
    "custom_style = \"\"\"\n",
    "<style>\n",
    "    td:nth-child(7) {text-align: left;}\n",
    "</style>\n",
    "\"\"\"\n",
    "html_table = metrics_df.to_html(index=False, max_rows=None, max_cols=None, notebook=True, escape=False)\n",
    "styled_html = f\"{custom_style}{html_table}\"\n",
    "\n",
    "# Display the styled HTML table\n",
    "display(HTML(styled_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the expected column exists in df\n",
    "if 'expected' not in df.columns:\n",
    "    raise KeyError(\"The 'expected' column does not exist in the DataFrame.\")\n",
    "\n",
    "# Count the occurrences of each service number in the 'expected' column\n",
    "service_number_counts = df['expected'].value_counts().reset_index()\n",
    "service_number_counts.columns = ['service_number', 'count']\n",
    "\n",
    "# Render the counts as an HTML table\n",
    "custom_style = \"\"\"\n",
    "<style>\n",
    "    td:nth-child(1), td:nth-child(2) {text-align: left;} /* Align service number and count to the left */\n",
    "</style>\n",
    "\"\"\"\n",
    "html_table = service_number_counts.to_html(index=False, max_rows=None, max_cols=None, notebook=True, escape=False)\n",
    "styled_html = f\"{custom_style}{html_table}\"\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nMost Frequent Service Numbers in the 'expected' Column:\")\n",
    "display(HTML(styled_html))\n",
    "\n",
    "# Plot the top 10 most frequent service numbers\n",
    "top_10 = service_number_counts.head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10['service_number'].astype(str), top_10['count'], color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Top 10 Most Frequent Service Numbers', fontsize=14)\n",
    "plt.xlabel('Service Number', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Function to find similarity between strings\n",
    "def find_common_prefix(strings, min_length=10):\n",
    "    \"\"\"Find the common prefix across a list of strings.\"\"\"\n",
    "    if not strings:\n",
    "        return \"\"\n",
    "    common_prefix = strings[0]\n",
    "    for s in strings[1:]:\n",
    "        matcher = SequenceMatcher(None, common_prefix, s)\n",
    "        match = matcher.find_longest_match(0, len(common_prefix), 0, len(s))\n",
    "        if match.size < min_length:\n",
    "            return \"\"\n",
    "        common_prefix = common_prefix[match.a: match.a + match.size]\n",
    "    return common_prefix.strip()\n",
    "\n",
    "# Input service number to analyze\n",
    "service_number_to_analyze = input(\"Enter the service number to analyze: \").strip()\n",
    "\n",
    "if not service_number_to_analyze.isdigit():\n",
    "    print(\"Invalid input! Please enter a valid numeric service number.\")\n",
    "else:\n",
    "    # Filter the records to contain only those with the service number in the 'expected' column\n",
    "    filtered_records = df[df['expected'] == service_number_to_analyze]\n",
    "    \n",
    "    if filtered_records.empty:\n",
    "        print(f\"No records found for Service Number: {service_number_to_analyze}\")\n",
    "    else:\n",
    "        print(f\"\\nAnalyzing query patterns for Service Number: {service_number_to_analyze}\\n\")\n",
    "\n",
    "        # Group queries based on their front portion similarity\n",
    "        query_groups = defaultdict(list)\n",
    "        queries = filtered_records['query'].tolist()\n",
    "        \n",
    "        for query in queries:\n",
    "            grouped = False\n",
    "            for group_key, group_list in query_groups.items():\n",
    "                if SequenceMatcher(None, query, group_key).ratio() >= 0.8:\n",
    "                    group_list.append(query)\n",
    "                    grouped = True\n",
    "                    break\n",
    "            if not grouped:\n",
    "                query_groups[query] = [query]\n",
    "\n",
    "        # Summarize groups with common prefix and count\n",
    "        summarized_groups = []\n",
    "        for group_key, group_queries in query_groups.items():\n",
    "            common_prefix = find_common_prefix(group_queries)\n",
    "            summarized_groups.append({\n",
    "                \"Common Prefix\": common_prefix or group_key[:30],  # Fallback to part of the first query\n",
    "                \"Query Count\": len(group_queries)\n",
    "            })\n",
    "\n",
    "        grouped_df = pd.DataFrame(summarized_groups).sort_values(by=\"Query Count\", ascending=False)\n",
    "\n",
    "        # Render as an HTML table\n",
    "        custom_style = \"\"\"\n",
    "        <style>\n",
    "            td:nth-child(1), td:nth-child(2) {text-align: left;} /* Align columns to the left */\n",
    "        </style>\n",
    "        \"\"\"\n",
    "        html_table = grouped_df.to_html(index=False, max_rows=None, max_cols=None, notebook=True, escape=False)\n",
    "        styled_html = f\"{custom_style}{html_table}\"\n",
    "\n",
    "        display(HTML(styled_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f86ed",
   "metadata": {},
   "source": [
    "Insights:<br>\n",
    "\n",
    "**High-Performing Service Numbers:**\n",
    "\n",
    "Service Numbers 212901 (Rental-Other Assets) and 215201 (Travel & Transport-Local) stand out with high precision values of 0.929 and 0.897, respectively.\n",
    "These service numbers have relatively low hit_rate, suggesting that while they appear less frequently in the top 5 matches, they are usually correct when they do appear.\n",
    "High precision with low hit_rate could indicate strong alignment between these service numbers and specific query types.<br>\n",
    "\n",
    "**Moderate Precision with High Hit Rate:**\n",
    "\n",
    "217501 (Tech Services-ICT Security/Audit Services) and 213201 (Service for Meals & Refreshments) have higher hit_rate but moderate precision (0.528 and 0.440, respectively).\n",
    "This suggests these service numbers are more frequently retrieved but are less consistently correct.\n",
    "They might be over-generalized and retrieved for queries they don't align well with.<br>\n",
    "\n",
    "\n",
    "**Low Precision with High Hit Rate:**\n",
    "\n",
    "Low precision in service numbers like 218401 (TOther Services-Postage and Courier Services) suggests potential issues with model embeddings or similarity thresholds.\n",
    "They may be retrieved for queries that align poorly with their descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c959952",
   "metadata": {},
   "source": [
    "**Query analysis of selected service number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Function to clean and extract meaningful keywords\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Extract meaningful keywords by removing stop words and non-alphabetic characters.\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Extract words\n",
    "    meaningful_words = [word for word in words if word not in ENGLISH_STOP_WORDS]  # Exclude stop words\n",
    "    return meaningful_words\n",
    "\n",
    "# Input the service number\n",
    "service_number_to_analyze = input(\"Enter the service number to analyze: \").strip()\n",
    "\n",
    "if not service_number_to_analyze.isdigit():\n",
    "    print(\"Invalid input! Please enter a valid numeric service number.\")\n",
    "else:\n",
    "    service_queries = df_copy[df_copy['retrieved_material_number'] == service_number_to_analyze]\n",
    "    \n",
    "    if service_queries.empty:\n",
    "        print(f\"No queries found for Service Number: {service_number_to_analyze}\")\n",
    "    else:\n",
    "        # Calculate query lengths\n",
    "        service_queries['query_length'] = service_queries['query'].str.len()\n",
    "\n",
    "        # Calculate metrics\n",
    "        total_correct = service_queries['is_correct'].sum()\n",
    "        total_incorrect = len(service_queries) - total_correct\n",
    "        accuracy = total_correct / len(service_queries) if len(service_queries) > 0 else 0\n",
    "\n",
    "        # Display overall metrics\n",
    "        print(f\"Total Correct: {total_correct}\")\n",
    "        print(f\"Total Incorrect: {total_incorrect}\")\n",
    "        print(f\"Accuracy: {accuracy:.3f}\\n\")\n",
    "\n",
    "        # Sort by correctness and similarity score\n",
    "        service_queries = service_queries.sort_values(by=['is_correct', 'score'], ascending=[True, False])\n",
    "        service_query_details = service_queries[['query', 'query_length', 'expected_description', \n",
    "                                                 'retrieved_description', 'score', 'is_correct']]\n",
    "\n",
    "        # Render table with custom CSS for alignment\n",
    "        custom_style = \"\"\"\n",
    "        <style>\n",
    "            td:nth-child(1), td:nth-child(3), td:nth-child(4) {text-align: left;} /* Align text columns to the left */\n",
    "        </style>\n",
    "        \"\"\"\n",
    "        html_table = service_query_details.to_html(index=False, max_rows=None, max_cols=None, notebook=True, escape=False)\n",
    "        styled_html = f\"{custom_style}{html_table}\"\n",
    "        \n",
    "        # Plot similarity score distribution\n",
    "        plt.hist(service_queries['score'], bins=range(0, 101, 5), color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        plt.title(f\"Similarity Score Distribution for Service Number {service_number_to_analyze}\", fontsize=14)\n",
    "        plt.xlabel(\"Similarity Score\", fontsize=12)\n",
    "        plt.ylabel(\"Frequency\", fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.xlim(0, 100)  \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot query lengths vs. similarity scores\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(service_queries[service_queries['is_correct']]['query_length'],\n",
    "                    service_queries[service_queries['is_correct']]['score'],\n",
    "                    color='blue', label='Correct', alpha=0.7, edgecolors='k')\n",
    "        plt.scatter(service_queries[~service_queries['is_correct']]['query_length'],\n",
    "                    service_queries[~service_queries['is_correct']]['score'],\n",
    "                    color='red', label='Incorrect', alpha=0.7, edgecolors='k')\n",
    "\n",
    "        # Add legend and labels\n",
    "        plt.title(f\"Query Length vs. Similarity Score for Service Number {service_number_to_analyze}\", fontsize=14)\n",
    "        plt.xlabel(\"Query Length\", fontsize=12)\n",
    "        plt.ylabel(\"Similarity Score\", fontsize=12)\n",
    "        plt.legend(title=\"Match Status\", fontsize=10, loc=\"upper right\")\n",
    "        plt.grid(alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Keyword Analysis\n",
    "        correct_queries = service_queries[service_queries['is_correct']]['query'].apply(lambda q: set(extract_keywords(q)))\n",
    "        incorrect_queries = service_queries[~service_queries['is_correct']]['query'].apply(lambda q: set(extract_keywords(q)))\n",
    "\n",
    "        # Flatten the sets and count frequencies\n",
    "        correct_keywords = Counter([kw for query_keywords in correct_queries for kw in query_keywords])\n",
    "        incorrect_keywords = Counter([kw for query_keywords in incorrect_queries for kw in query_keywords])\n",
    "\n",
    "        # Create a DataFrame for keyword comparison\n",
    "        keywords_df = pd.DataFrame({\n",
    "            \"keyword\": list(set(correct_keywords.keys()).union(set(incorrect_keywords.keys()))),\n",
    "            \"correct_count\": [correct_keywords.get(k, 0) for k in set(correct_keywords.keys()).union(set(incorrect_keywords.keys()))],\n",
    "            \"incorrect_count\": [incorrect_keywords.get(k, 0) for k in set(correct_keywords.keys()).union(set(incorrect_keywords.keys()))]\n",
    "        }).sort_values(by=\"correct_count\", ascending=False)\n",
    "\n",
    "        # Display keyword analysis table\n",
    "        service_description = service_queries[\"retrieved_description\"].iloc[0]  # Get the description of the service number\n",
    "        print(f\"\\nKeyword Analysis for Service Number: {service_number_to_analyze}\")\n",
    "        print(f\"Description: {service_description}\\n\")\n",
    "        display(HTML(keywords_df.head(20).to_html(index=False, max_rows=None)))\n",
    "\n",
    "        # Plot top 10 meaningful keywords for correct and incorrect queries\n",
    "        top_keywords = keywords_df.head(10)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bar_width = 0.35\n",
    "        index = range(len(top_keywords))\n",
    "\n",
    "        plt.bar(index, top_keywords['correct_count'], bar_width, label='Correct', color='blue', alpha=0.7)\n",
    "        plt.bar([i + bar_width for i in index], top_keywords['incorrect_count'], bar_width, label='Incorrect', color='red', alpha=0.7)\n",
    "\n",
    "        plt.xlabel('Keywords', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.title(f\"Top Keywords for Service Number {service_number_to_analyze}\", fontsize=14)\n",
    "        plt.xticks([i + bar_width / 2 for i in index], top_keywords['keyword'], rotation=45)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Display the table\n",
    "        print(f\"\\nAnalysis of Queries for Service Number: {service_number_to_analyze}\")\n",
    "        display(HTML(styled_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9bb91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "338891f5",
   "metadata": {},
   "source": [
    "**Code below to export results to excel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5794c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare data for export\n",
    "# additional_data = []\n",
    "\n",
    "# for result in results:  \n",
    "#     # Combine expected material number and description\n",
    "#     expected_combined = f\"{result['expected']} - {result['expected_description']}\"\n",
    "\n",
    "#     # Combine top 5 matched material numbers into a single string\n",
    "#     top_matches_combined = \"; \".join(\n",
    "#         [f\"{match['material_number']} score:{match['score']}\" for match in result[\"retrieved_top_5\"]]\n",
    "#     )\n",
    "\n",
    "#     additional_data.append({\n",
    "#         \"query\": result[\"query\"],\n",
    "#         \"expected\": expected_combined,  \n",
    "#         \"matches\": top_matches_combined,  \n",
    "#         \"is_correct\": \"True\" if result[\"is_correct\"] else \"False\"  \n",
    "#     })\n",
    "\n",
    "# additional_df = pd.DataFrame(additional_data)\n",
    "\n",
    "# output_path = \"../data/output/evaluation_results.xlsx\"\n",
    "\n",
    "# # Dynamically get the model name\n",
    "# model_name = getattr(search_engine, \"model_name\", \"Unknown_Model\")\n",
    "# sheet_name = model_name.replace(\"/\", \"_\")[:31]  \n",
    "\n",
    "# os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# # Append to the Excel file\n",
    "# with pd.ExcelWriter(output_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "#     additional_df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
    "\n",
    "# print(f\"Additional results appended to sheet '{sheet_name}' in: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843dddd-462d-4dba-b56b-8456aeb0214b",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
